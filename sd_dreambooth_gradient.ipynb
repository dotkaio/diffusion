{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notice:\n",
    "To train Dreambooth Stable Diffusion requires a recommended minimum of 16 GB of GPU RAM. Trying to train Dreambooth with less will trigger an Out of Memory error. This Notebook is defaulted to set up to run on the RTX5000 GPU, which requires a paid account on Gradient. \n",
    "\n",
    "Change the URL at the top where it says \"machine?=RTX5000\" to any GPU with at least 16 GB of RAM to run training. The P6000, V100, V100-32G, RTX5000, A4000, A5000, A100, and A100-80G powered machines will all be able to run this training. \n",
    "\n",
    "You can do so by changing the following URL where it says \"(YOUR-GPU-CHOICE)\":\n",
    "\n",
    " https://console.paperspace.com/github/gradient-ai/dreambooth-stable-diffusion/blob/main/sd_dreambooth_gradient.ipynb?machine=(YOUR-GPU-CHOICE)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dreambooth fine-tuning for Stable Diffusion using dðŸ§¨ffusers with Gradient Notebooks\n",
    "\n",
    "This notebook shows how to \"teach\" Stable Diffusion a new concept via Dreambooth using ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers). \n",
    "\n",
    "![Dreambooth Example](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)\n",
    "_By using just 3-5 images you can teach new concepts to Stable Diffusion and personalize the model on your own images_ \n",
    "\n",
    "Differently from Textual Inversion, this approach trains the whole model, which can yield better results to the cost of bigger models.\n",
    "\n",
    "For a general introduction to the Stable Diffusion model please refer to this [Gradient Notebook](https://console.paperspace.com/github/gradient-ai/stable-diffusion/blob/main/stable-diffusion-notebook.ipynb?machine=A4000).\n",
    "\n"
   ],
   "metadata": {
    "id": "tAZq3vFDcFiT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation"
   ],
   "metadata": {
    "id": "KbzZ9xe6dWwf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title Install the required libs\n",
    "!pip install -qq accelerate tensorboard ftfy\n",
    "!pip install -qq --upgrade diffusers[torch] --no-cache-dir\n",
    "!pip install -qq -U transformers\n",
    "!pip install -qq bitsandbytes\n",
    "!pip install gradio\n",
    "!mkdir inputs\n",
    "!git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T17:34:36.733188Z",
     "iopub.status.busy": "2022-10-28T17:34:36.732409Z",
     "iopub.status.idle": "2022-10-28T17:34:46.339367Z",
     "shell.execute_reply": "2022-10-28T17:34:46.338666Z",
     "shell.execute_reply.started": "2022-10-28T17:34:36.733122Z"
    },
    "id": "30lu8LWXmg5j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading in the Stable Diffusion v1-5 models\n",
    "\n",
    "To make accessing the Stable Diffusion models easy and not take up any storage, we have added the Stable Diffusion models v1-5 as mountable public datasets! To access the models this way, simply navigate to the \"Data Sources\" tab using the navigator on the far left of the page. Then click \"Public\" to switch into the Gradient Public Datasets, and scroll down until you find \"stable-diffusion\" near the bottom of the list. Then, click \"mount\" to make them accessible from the \"datasets\" directory. This directory is in the root folder, so access it with the path `../datasets/stable-diffusion-diffusers/stable-diffusion-v1-5`\n",
    "\n",
    "> Note that these public dataset files will not count toward the file storage limits \n",
    "\n",
    "\n",
    "<video controls src=\"assets/stab-upload.mp4\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Alternate access: log in to HuggingFace for online access to models\n",
    "\n",
    "In order to access the models from CompVis for Stable Diffusion, you must follow three steps:\n",
    "\n",
    "1. You must acknowledge and agree to their user requirements and license for their models. you can do so by reading the instructions found on this page: https://huggingface.co/runwayml/stable-diffusion-v1-5\n",
    "\n",
    "2. You must login to Huggingface, and then create and retrieve an access token (found here: https://huggingface.co/settings/tokens)\n",
    "\n",
    "3. Finally, replace the segment of the cell below `<your_huggingface_token>` with your own token, and run the cell. \n",
    "\n",
    "If you follow these steps, you will be able to access the model for free!\n",
    "\n",
    "If you are using this method, be sure to change the path to `runwayml/stable-diffusion-v1-5`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !wget https://raw.githubusercontent.com/gradient-ai/stable-diffusion/main/login.py\n",
    "# !python login.py --token <your_huggingface_token>"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T17:34:46.340804Z",
     "iopub.status.busy": "2022-10-28T17:34:46.340614Z",
     "iopub.status.idle": "2022-10-28T17:34:46.758162Z",
     "shell.execute_reply": "2022-10-28T17:34:46.757334Z",
     "shell.execute_reply.started": "2022-10-28T17:34:46.340786Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title Import required libraries\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import PIL\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "\n",
    "# Helper function\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-28T17:39:42.305061Z",
     "iopub.status.busy": "2022-10-28T17:39:42.304381Z",
     "iopub.status.idle": "2022-10-28T17:39:44.179822Z",
     "shell.execute_reply": "2022-10-28T17:39:44.179165Z",
     "shell.execute_reply.started": "2022-10-28T17:39:42.304996Z"
    },
    "id": "1_h0kO-VnQog",
    "outputId": "96e5e3cf-0da9-4a0c-b506-35de5e724d8d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings for teaching your new concept\n",
    "\n",
    "When setting up your data, you can use the demo data provided below to create your first inversion. \n",
    "\n",
    "### Display your video output in markdown\n",
    "If you would like to use your own images, hash out the cells below and add them to the `inputs` directory.\n",
    "\n",
    "<video controls src=\"assets/upload.mp4\"/>\n"
   ],
   "metadata": {
    "id": "Yl3r7A_3ASxm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@markdown Add here the URLs to the images of the concept you are adding. 3-5 should be fine\n",
    "urls = [\n",
    "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/2.jpeg\",\n",
    "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/3.jpeg\",\n",
    "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/5.jpeg\",\n",
    "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/6.jpeg\",\n",
    "      ## You can add additional images here\n",
    "      ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# @title Setup and check the images you have just added\n",
    "import requests\n",
    "import glob\n",
    "from io import BytesIO\n",
    "\n",
    "def download_image(url):\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "  except:\n",
    "    return None\n",
    "  return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "images = list(filter(None,[download_image(url) for url in urls]))\n",
    "save_path = \"./inputs\"\n",
    "if not os.path.exists(save_path):\n",
    "  os.mkdir(save_path)\n",
    "[image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]\n",
    "image_grid(images, 1, len(images))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set training variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#@title Settings for your newly created concept\n",
    "#@markdown `instance_prompt` is a prompt that should contain a good description of what your object or style is, together with the initializer word `sks`  \n",
    "instance_prompt = \"a photo of sks toy\" #@param {type:\"string\"}\n",
    "#@markdown Check the `prior_preservation` option if you would like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\n",
    "prior_preservation = False #@param {type:\"boolean\"}\n",
    "prior_preservation_class_prompt = \"a photo of a cat toy\" #@param {type:\"string\"}\n",
    "class_prompt=prior_preservation_class_prompt\n",
    "\n",
    "# Set your data folder path \n",
    "prior_preservation_class_folder = './inputs'\n",
    "class_data_root=prior_preservation_class_folder\n",
    "\n",
    "num_class_images = len(os.listdir(prior_preservation_class_folder))\n",
    "sample_batch_size = 4\n",
    "prior_loss_weight = 0.5"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-10-28T17:39:48.402533Z",
     "iopub.status.busy": "2022-10-28T17:39:48.402016Z",
     "iopub.status.idle": "2022-10-28T17:39:48.405711Z",
     "shell.execute_reply": "2022-10-28T17:39:48.405194Z",
     "shell.execute_reply.started": "2022-10-28T17:39:48.402512Z"
    },
    "id": "8i_vLTBxAXpE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Advanced settings for prior preservation (optional)"
   ],
   "metadata": {
    "id": "NCuPKFeMLe9S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mkdir class_images"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#@markdown If the `prior_preservation_class_folder` is empty, images for the class will be generated with the class prompt. Otherwise, fill this folder with images of items on the same class as your concept (but not images of the concept itself)\n",
    "prior_preservation_class_folder = \"./class_images\" #@param {type:\"string\"}\n",
    "class_data_root=prior_preservation_class_folder\n",
    "\n",
    "num_class_images = len(os.listdir(prior_preservation_class_folder)) #@param {type: \"number\"}\n",
    "sample_batch_size = 2\n",
    "#@markdown `prior_preservation_weight` determins how strong the class for prior preservation should be \n",
    "prior_loss_weight = 1 #@param {type: \"number\"}"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T03:05:57.703257Z",
     "iopub.status.busy": "2022-10-28T03:05:57.702995Z",
     "iopub.status.idle": "2022-10-28T03:05:57.706530Z",
     "shell.execute_reply": "2022-10-28T03:05:57.706053Z",
     "shell.execute_reply.started": "2022-10-28T03:05:57.703238Z"
    },
    "id": "y8SQ259CK3Cd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate Dataset classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Setup the Classes\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.instance_data_root = Path(instance_data_root)\n",
    "        if not self.instance_data_root.exists():\n",
    "            raise ValueError(\"Instance images root doesn't exists.\")\n",
    "\n",
    "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
    "            self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                self.class_prompt,\n",
    "                padding=\"do_not_pad\",\n",
    "                truncation=True,\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "            ).input_ids\n",
    "        \n",
    "        return example\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Generate Class Images "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#@title Generate Class Images\n",
    "import gc\n",
    "if(prior_preservation):\n",
    "    class_images_dir = Path(class_data_root)\n",
    "    if not class_images_dir.exists():\n",
    "        class_images_dir.mkdir(parents=True)\n",
    "    cur_class_images = len(list(class_images_dir.iterdir()))\n",
    "\n",
    "    if cur_class_images < num_class_images:\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            pretrained_model_name_or_path, revision=\"fp16\", torch_dtype=torch.float16\n",
    "        ).to(\"cuda\")\n",
    "        pipeline.enable_attention_slicing()\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "        num_new_images = num_class_images - cur_class_images\n",
    "        print(f\"Number of class images to sample: {num_new_images}.\")\n",
    "\n",
    "        sample_dataset = PromptDataset(class_prompt, num_new_images)\n",
    "        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n",
    "\n",
    "        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n",
    "            images = pipeline(example[\"prompt\"]).images\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n",
    "        pipeline = None\n",
    "        gc.collect()\n",
    "        del pipeline\n",
    "        with torch.no_grad():\n",
    "          torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-10-28T17:39:54.239762Z",
     "iopub.status.busy": "2022-10-28T17:39:54.239169Z",
     "iopub.status.idle": "2022-10-28T17:39:54.245279Z",
     "shell.execute_reply": "2022-10-28T17:39:54.244738Z",
     "shell.execute_reply.started": "2022-10-28T17:39:54.239741Z"
    },
    "id": "mneZ4Ct2BenE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load in your model checkpoints\n",
    "\n",
    "You can either use the Stable Diffusion v1-5 or v2 checkpoints here. Use the 2 cells below for v1-5, and the 2 cells below them for v2.\n",
    "\n",
    "### Stable Diffusion v1-5 or v2\n",
    "\n",
    "Hash out the line below you would not like to use. Default is v2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Load the Stable Diffusion model\n",
    "# Load models and create wrapper for stable diffusion\n",
    "#@markdown `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use\n",
    "## Use local files\n",
    "\n",
    "## V1-5\n",
    "pretrained_model_name_or_path = \"../datasets/stable-diffusion-diffusers/stable-diffusion-v1-5/\" #@param {type:\"string\"}\n",
    "\n",
    "##v2\n",
    "# pretrained_model_name_or_path = '../datasets/stable-diffusion-diffusers-v2/stable-diffusion-2/'\n",
    "\n",
    "\n",
    "\n",
    "## Download online files\n",
    "#@markdown Please read and, if you agree, accept the LICENSE [here](https://huggingface.co/runwayml/stable-diffusion-v1-5) if you see an error\n",
    "# pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path+'text_encoder', local_files_only = True,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path+\"vae\", local_files_only = True, \n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path+\"unet\", local_files_only = True,\n",
    ")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path+\"tokenizer\", local_files_only = True)"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-10-28T17:39:55.137167Z",
     "iopub.status.busy": "2022-10-28T17:39:55.136421Z",
     "iopub.status.idle": "2022-10-28T17:41:13.203886Z",
     "shell.execute_reply": "2022-10-28T17:41:13.203233Z",
     "shell.execute_reply.started": "2022-10-28T17:39:55.137146Z"
    },
    "id": "gIFaJum5nqeo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up args for training and output directory as save_path"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We create an outputs directory here, but you can use any directory by changing `save_path` in the cell below\n",
    "!mkdir outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#@title Setting up all training args\n",
    "save_path = 'outputs'\n",
    "from argparse import Namespace\n",
    "if pretrained_model_name_or_path == '../datasets/stable-diffusion-diffusers-v2/stable-diffusion-2/':\n",
    "    args = Namespace(\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        resolution=768,\n",
    "        center_crop=True,\n",
    "        instance_data_dir=save_path,\n",
    "        instance_prompt=instance_prompt,\n",
    "        learning_rate=5e-06,\n",
    "        max_train_steps=500,\n",
    "        train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_grad_norm=1.0,\n",
    "        mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
    "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
    "        use_8bit_adam=True, # use 8bit optimizer from bitsandbytes\n",
    "        seed=34354,\n",
    "        with_prior_preservation=prior_preservation, \n",
    "        prior_loss_weight=prior_loss_weight,\n",
    "        sample_batch_size=2,\n",
    "        class_data_dir=prior_preservation_class_folder, \n",
    "        class_prompt=None, \n",
    "        num_class_images=num_class_images, \n",
    "        output_dir=\"dreambooth-concept\",\n",
    "    )\n",
    "else:\n",
    "    args = Namespace(\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        resolution=512,\n",
    "        center_crop=True,\n",
    "        instance_data_dir=save_path,\n",
    "        instance_prompt=instance_prompt,\n",
    "        learning_rate=5e-06,\n",
    "        max_train_steps=500,\n",
    "        train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_grad_norm=1.0,\n",
    "        mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
    "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
    "        use_8bit_adam=True, # use 8bit optimizer from bitsandbytes\n",
    "        seed=34354,\n",
    "        with_prior_preservation=prior_preservation, \n",
    "        prior_loss_weight=prior_loss_weight,\n",
    "        sample_batch_size=2,\n",
    "        class_data_dir=prior_preservation_class_folder, \n",
    "        class_prompt=None, \n",
    "        num_class_images=num_class_images, \n",
    "        output_dir=\"dreambooth-concept\",\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T17:57:45.104035Z",
     "iopub.status.busy": "2022-10-28T17:57:45.103735Z",
     "iopub.status.idle": "2022-10-28T17:57:45.108819Z",
     "shell.execute_reply": "2022-10-28T17:57:45.108252Z",
     "shell.execute_reply.started": "2022-10-28T17:57:45.104015Z"
    },
    "id": "r4ayDcwEHDa4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define training function with accelerate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#@title Training function\n",
    "from accelerate.utils import set_seed\n",
    "def training_function(text_encoder, vae, unet):\n",
    "    logger = get_logger(__name__)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "    )\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if args.use_8bit_adam:\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        unet.parameters(),  # only optimize unet\n",
    "        lr=args.learning_rate,\n",
    "    )\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
    "    )\n",
    "    \n",
    "    train_dataset = DreamBoothDataset(\n",
    "        instance_data_root='inputs',\n",
    "        instance_prompt=args.instance_prompt,\n",
    "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
    "        class_prompt=class_prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        size=args.resolution,\n",
    "        center_crop=args.center_crop,\n",
    "    )\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "\n",
    "        # concat class and instance examples for prior preservation\n",
    "        if args.with_prior_preservation:\n",
    "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "            pixel_values += [example[\"class_images\"] for example in examples]\n",
    "\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": pixel_values,\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
    "\n",
    "    # Move text_encode and vae to gpu\n",
    "    text_encoder.to(accelerator.device)\n",
    "    vae.to(accelerator.device)\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "  \n",
    "    # Train!\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn(latents.shape).to(latents.device)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
    "                ).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                # Predict the noise residual\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "                if args.with_prior_preservation:\n",
    "                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
    "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
    "                    noise, noise_prior = torch.chunk(noise, 2, dim=0)\n",
    "\n",
    "                    # Compute instance loss\n",
    "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "\n",
    "                    # Compute prior loss\n",
    "                    prior_loss = F.mse_loss(noise_pred_prior, noise_prior, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "\n",
    "                    # Add the prior loss to the instance loss.\n",
    "                    loss = loss + args.prior_loss_weight * prior_loss\n",
    "                else:\n",
    "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "    \n",
    "    # Create the pipeline using using the trained modules and save it.\n",
    "    if accelerator.is_main_process:\n",
    "        pipeline = StableDiffusionPipeline(\n",
    "            text_encoder=text_encoder,\n",
    "            vae=vae,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=PNDMScheduler(\n",
    "                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n",
    "            ),\n",
    "            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "        )\n",
    "        pipeline.save_pretrained(args.output_dir)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T17:57:46.108764Z",
     "iopub.status.busy": "2022-10-28T17:57:46.107958Z",
     "iopub.status.idle": "2022-10-28T17:57:46.123615Z",
     "shell.execute_reply": "2022-10-28T17:57:46.122793Z",
     "shell.execute_reply.started": "2022-10-28T17:57:46.108743Z"
    },
    "id": "1lKGmcIyJbCu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Teach the model the new concept (fine-tuning with Dreambooth)\n",
    "Execute this this sequence of cells to run the training process. The whole process may take from 15 min to 2 hours. (Open this block if you are interested in how this process works under the hood or if you want to change advanced training settings or hyperparameters)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Run training\n",
    "import accelerate\n",
    "accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet), num_processes =1)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-10-28T17:57:47.251675Z",
     "iopub.status.busy": "2022-10-28T17:57:47.250928Z",
     "iopub.status.idle": "2022-10-28T18:14:02.721817Z",
     "shell.execute_reply": "2022-10-28T18:14:02.721058Z",
     "shell.execute_reply.started": "2022-10-28T17:57:47.251654Z"
    },
    "id": "mfeKJn_LJi_V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up the inference pipeline\n",
    "\n",
    "Use this pipeline to generate images from your new model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#@title Set up the pipeline \n",
    "try:\n",
    "    pipe\n",
    "except NameError:\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        args.output_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\")"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-10-28T18:14:15.533635Z",
     "iopub.status.busy": "2022-10-28T18:14:15.533026Z",
     "iopub.status.idle": "2022-10-28T18:14:15.539748Z",
     "shell.execute_reply": "2022-10-28T18:14:15.539200Z",
     "shell.execute_reply.started": "2022-10-28T18:14:15.533609Z"
    },
    "id": "2CMlPbOeEC09"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#@title Run the Stable Diffusion pipeline with interactive UI Demo on Gradio\n",
    "#@markdown Run this cell to get an interactive demo where you can run the model using Gradio\n",
    "\n",
    "#@markdown ![](https://i.imgur.com/2ACLWu2.png)\n",
    "import gradio as gr\n",
    "\n",
    "def inference(prompt, num_samples):\n",
    "    all_images = [] \n",
    "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
    "    all_images.extend(images)\n",
    "    return all_images\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"prompt\")\n",
    "            samples = gr.Slider(label=\"Samples\",value=1)\n",
    "            run = gr.Button(value=\"Run\")\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery(show_label=False)\n",
    "\n",
    "    run.click(inference, inputs=[prompt,samples], outputs=gallery)\n",
    "    gr.Examples([[\"a photo of sks toy riding a bicycle\", 1,1]], [prompt,samples], gallery, inference, cache_examples=False)\n",
    "\n",
    "\n",
    "## Unhash the line below to launch your demo in Gradio\n",
    "\n",
    "# demo.launch()\n"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-10-28T18:14:18.612351Z",
     "iopub.status.busy": "2022-10-28T18:14:18.611700Z",
     "iopub.status.idle": "2022-10-28T18:14:21.010823Z",
     "shell.execute_reply": "2022-10-28T18:14:21.010090Z",
     "shell.execute_reply.started": "2022-10-28T18:14:18.612330Z"
    },
    "id": "rscg285SBh4M"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Run the Stable Diffusion pipeline to generate quick samples in the Notebook\n",
    "\n",
    "prompt = \"a photo of a sks cat toy riding a bicycle\" #@param {type:\"string\"}\n",
    "\n",
    "num_samples = 3  #@param {type:\"number\"}\n",
    "num_rows = 3 #@param {type:\"number\"}\n",
    "\n",
    "all_images = [] \n",
    "for _ in range(num_rows):\n",
    "    images = pipe([prompt] * num_samples, num_inference_steps=75, guidance_scale=7.5, seed = 'random').images\n",
    "    all_images.extend(images)\n",
    "\n",
    "grid = image_grid(all_images, num_samples, num_rows)\n",
    "grid "
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593,
     "referenced_widgets": [
      "f54f189db8614f28a5bc262a3dda4006",
      "3fc16e2f87f041d282b708b67a443875",
      "80879a82806b4be8a0f6a4ac111026df",
      "531fccae2fd741dab7b02f4a1ac17f21",
      "9c6d5ee3e13d449296a13096ea81f7db",
      "2f6dd40cc3d34b1c934b186ff1cb1bc5",
      "0862d6ac9a774d958b07f5b3c7d39726",
      "16e498bcf934417fb737ff32bfd00f57",
      "e8258f8326554c958923abd3323a33d6",
      "5135e0a705fa4a93a929546feed88f0d",
      "01b6cb52e86a4b69ae11c0a10d7688f8",
      "302c0ec13bc84d7c9d328e93b046b4fd",
      "79c86255caf1493d8660ace4fa5738cb",
      "576424660f224261b366d713967deb94",
      "fb70a01286694e1fb40dfd6c940728e1",
      "826d782955cd4dc0bd56c81c5d944330",
      "5994e37481214ea7aa27415b7c4edc43",
      "e4a4ce8dba4a40bbb246eebb3f3aaef7",
      "8d63afe1f5ee4b6a9951058b419d8059",
      "333cc2b099274f25af41f54753ba9305",
      "5543bfa403a14af7a572f943d0359336",
      "e615aa96bb8d461cb6da7384d8180737"
     ]
    },
    "execution": {
     "iopub.execute_input": "2022-10-28T18:14:21.012435Z",
     "iopub.status.busy": "2022-10-28T18:14:21.012085Z",
     "iopub.status.idle": "2022-10-28T18:15:28.463076Z",
     "shell.execute_reply": "2022-10-28T18:15:28.461615Z",
     "shell.execute_reply.started": "2022-10-28T18:14:21.012418Z"
    },
    "id": "E3UREGd7EkLh",
    "outputId": "ec3cfc87-832d-4646-88de-68451a0601fc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert your concept to a Stable Diffusion Checkpoint\n",
    "Use the cell below to convert your model to classic Stable Diffusion `.ckpt` format. This can then be used with any Stable Diffusion fork, such as the Web UI available to run from the Gradient Notebook Stable Diffusion Runtime tile. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Params for `convert_diffusers_to_original_stable_diffusion.py`:\n",
    "# --model_path = Path to the model directory (default \"dreambooth-concept\") to convert.\n",
    "# --checkpoint_path = Path to the output model.\n",
    "# --half = store_true, Save weights in half precision/\n",
    "\n",
    "# Get script and convert model\n",
    "%cd ~/../notebooks\n",
    "!git clone https://github.com/huggingface/diffusers\n",
    "\n",
    "# template: use this if you have a custom concept\n",
    "# !python diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py --model_path <path to dreambooth concept> --checkpoint_path <name/path of your new model>\n",
    "\n",
    "\n",
    "# Send our \"dreambooth-concept\" directory as .ckpt to the Web UI\n",
    "!python diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py --model_path dreambooth-concept/ --checkpoint_path stable-diffusion-webui/models/Stable-diffusion/db-concept.ckpt\n",
    "\n",
    "%cd ~/../notebooks\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use the Stable Diffusion Web UI\n",
    "\n",
    "Now that our new checkpoint file is in the right place, we can run the cell below to setup and launch the Automatic1111 Stable Diffusion Web UI from this Notebook. Click the Gradio link after the setup completes to access the Web UI in your local browser. \n",
    "\n",
    "> Note: Be sure to check out the Textual Inversion notebook before continuing!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading in the Stable Diffusion v1-5 models for the Web UI \n",
    "\n",
    "To make accessing the Stable Diffusion models easy and not take up any storage, we have added the Stable Diffusion models v1-5 as mountable public datasets! \n",
    "\n",
    "First, navigate to the \"Data Sources\" tab using the navigator on the far left of the page.\n",
    "\n",
    "Next, click \"Public\" to switch into the Gradient Public Datasets, and scroll down until you find \"stable-diffusion-classic\" near the bottom of the list. \n",
    "\n",
    "Finally, click \"mount\" to make them accessible from the \"datasets\" directory. This directory is in the root folder, so access it with the path `~/../datasets/stable-diffusion-classic/v1-5-pruned-emaonly.ckpt`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd stable-diffusion-webui\n",
    "# launch the webui\n",
    "!python launch.py --share --ckpt ../../datasets/stable-diffusion-classic/v1-5-pruned-emaonly.ckpt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Upload model to DreamBooth Concepts Library\n",
    "\n",
    "Also explore the [DreamBooth Concepts Library](https://huggingface.co/sd-dreambooth-library) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Save your newly created concept? you may save it privately to your personal profile or collaborate to the [library of concepts](https://huggingface.co/sd-dreambooth-library)?\n",
    "# #@markdown If you wish your model to be avaliable for everyone, add it to the public library. If you prefer to use your model privately, add your own profile.\n",
    "\n",
    "# save_concept = True #@param {type:\"boolean\"}\n",
    "# #@markdown Once you save it you can use your concept by loading the model on any `from_pretrained` function\n",
    "# name_of_your_concept = \"Cat toy\" #@param {type:\"string\"}\n",
    "# where_to_save_concept = \"public_library\" #@param [\"public_library\", \"privately_to_my_profile\"]\n",
    "\n",
    "# #@markdown `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)\n",
    "# hf_token_write = \"\" #@param {type:\"string\"}\n",
    "# if(save_concept):\n",
    "#   from slugify import slugify\n",
    "#   from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
    "#   from huggingface_hub import create_repo\n",
    "#   from IPython.display import display_markdown\n",
    "#   api = HfApi()\n",
    "#   your_username = api.whoami()[\"name\"]\n",
    "#   pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#   ).to(\"cuda\")\n",
    "#   os.makedirs(\"fp16_model\",exist_ok=True)\n",
    "#   pipe.save_pretrained(\"fp16_model\")\n",
    "\n",
    "#   if(where_to_save_concept == \"public_library\"):\n",
    "#     repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
    "#     #Join the Concepts Library organization if you aren't part of it already\n",
    "#     !curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\n",
    "#   else:\n",
    "#     repo_id = f\"{your_username}/{slugify(name_of_your_concept)}\"\n",
    "#   output_dir = args.output_dir\n",
    "#   if(not hf_token_write):\n",
    "#     with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
    "#   else:\n",
    "#     hf_token = hf_token_write \n",
    "  \n",
    "#   images_upload = os.listdir(\"my_concept\")\n",
    "#   image_string = \"\"\n",
    "#   #repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
    "#   for i, image in enumerate(images_upload):\n",
    "#       image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
    "# '''\n",
    "#   readme_text = f'''---\n",
    "# license: mit\n",
    "# ---\n",
    "# ### {name_of_your_concept} on Stable Diffusion via Dreambooth\n",
    "# #### model by {api.whoami()[\"name\"]}\n",
    "# This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.\n",
    "# It can be used by modifying the `instance_prompt`: **{instance_prompt}**\n",
    "\n",
    "# You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
    "# And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
    "\n",
    "# Here are the images used for training this concept:\n",
    "# {image_string}\n",
    "# '''\n",
    "#   #Save the readme to a file\n",
    "#   readme_file = open(\"README.md\", \"w\")\n",
    "#   readme_file.write(readme_text)\n",
    "#   readme_file.close()\n",
    "#   #Save the token identifier to a file\n",
    "#   text_file = open(\"token_identifier.txt\", \"w\")\n",
    "#   text_file.write(instance_prompt)\n",
    "#   text_file.close()\n",
    "#   operations = [\n",
    "#     CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
    "#     CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
    "#   ]\n",
    "#   create_repo(repo_id,private=True, token=hf_token)\n",
    "  \n",
    "#   api.create_commit(\n",
    "#     repo_id=repo_id,\n",
    "#     operations=operations,\n",
    "#     commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n",
    "#     token=hf_token\n",
    "#   )\n",
    "#   api.upload_folder(\n",
    "#     folder_path=\"fp16_model\",\n",
    "#     path_in_repo=\"\",\n",
    "#     repo_id=repo_id,\n",
    "#     token=hf_token\n",
    "#   )\n",
    "#   api.upload_folder(\n",
    "#     folder_path=save_path,\n",
    "#     path_in_repo=\"concept_images\",\n",
    "#     repo_id=repo_id,\n",
    "#     token=hf_token\n",
    "#   )\n",
    "# display_markdown(f'''## Your concept was saved successfully. [Click here to access it](https://huggingface.co/{repo_id})\n",
    "# ''', raw=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NCuPKFeMLe9S",
    "D633UIuGgs6M"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "76721e0cd9246c299eb22246d1f3c601ec1aef6bd84d45d2547549094e7b6fb7"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01b6cb52e86a4b69ae11c0a10d7688f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0862d6ac9a774d958b07f5b3c7d39726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a4f5827c6694322aac53eef58361be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "169f9d7cfb60460c91fa301dc07b88cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2f1bd58d757b4755bf6e43dcd1db7a8d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6407a100ee1a48fc946c7968160d8c11",
      "value": ""
     }
    },
    "16e498bcf934417fb737ff32bfd00f57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f1bd58d757b4755bf6e43dcd1db7a8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f6dd40cc3d34b1c934b186ff1cb1bc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "302c0ec13bc84d7c9d328e93b046b4fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79c86255caf1493d8660ace4fa5738cb",
       "IPY_MODEL_576424660f224261b366d713967deb94",
       "IPY_MODEL_fb70a01286694e1fb40dfd6c940728e1"
      ],
      "layout": "IPY_MODEL_826d782955cd4dc0bd56c81c5d944330"
     }
    },
    "318b6ba49b324a02be064792c61da1f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_329e8da97d0044aca10a821dc3154fea",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7b06663e76fd4e00abcaeb1aa5e0c415",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "329e8da97d0044aca10a821dc3154fea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "333cc2b099274f25af41f54753ba9305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3fc16e2f87f041d282b708b67a443875": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f6dd40cc3d34b1c934b186ff1cb1bc5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0862d6ac9a774d958b07f5b3c7d39726",
      "value": "100%"
     }
    },
    "45f2700e51eb49a98f305db44c378cf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_318b6ba49b324a02be064792c61da1f1",
       "IPY_MODEL_169f9d7cfb60460c91fa301dc07b88cc",
       "IPY_MODEL_cd65142ea5004475b34a830ccc7db6cd",
       "IPY_MODEL_7267876e603a400da469cb87f7c446c4"
      ],
      "layout": "IPY_MODEL_fea860f9ed9246189b71717b9b13be20"
     }
    },
    "5135e0a705fa4a93a929546feed88f0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "531fccae2fd741dab7b02f4a1ac17f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5135e0a705fa4a93a929546feed88f0d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01b6cb52e86a4b69ae11c0a10d7688f8",
      "value": " 51/51 [00:14&lt;00:00,  3.44it/s]"
     }
    },
    "5543bfa403a14af7a572f943d0359336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "576424660f224261b366d713967deb94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d63afe1f5ee4b6a9951058b419d8059",
      "max": 51,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_333cc2b099274f25af41f54753ba9305",
      "value": 51
     }
    },
    "5994e37481214ea7aa27415b7c4edc43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6407a100ee1a48fc946c7968160d8c11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7267876e603a400da469cb87f7c446c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce7210f7a79d449e8709fd700dda49ce",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0a4f5827c6694322aac53eef58361be4",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "79c86255caf1493d8660ace4fa5738cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5994e37481214ea7aa27415b7c4edc43",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e4a4ce8dba4a40bbb246eebb3f3aaef7",
      "value": "100%"
     }
    },
    "7b06663e76fd4e00abcaeb1aa5e0c415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80879a82806b4be8a0f6a4ac111026df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16e498bcf934417fb737ff32bfd00f57",
      "max": 51,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8258f8326554c958923abd3323a33d6",
      "value": 51
     }
    },
    "826d782955cd4dc0bd56c81c5d944330": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d63afe1f5ee4b6a9951058b419d8059": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c6d5ee3e13d449296a13096ea81f7db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a497ef1b787f40ec8dc6b523a15b7ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "a92e5369635b45ea97bc8238adf15c7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd65142ea5004475b34a830ccc7db6cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_a92e5369635b45ea97bc8238adf15c7c",
      "style": "IPY_MODEL_a497ef1b787f40ec8dc6b523a15b7ce5",
      "tooltip": ""
     }
    },
    "ce7210f7a79d449e8709fd700dda49ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4a4ce8dba4a40bbb246eebb3f3aaef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e615aa96bb8d461cb6da7384d8180737": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8258f8326554c958923abd3323a33d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f54f189db8614f28a5bc262a3dda4006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fc16e2f87f041d282b708b67a443875",
       "IPY_MODEL_80879a82806b4be8a0f6a4ac111026df",
       "IPY_MODEL_531fccae2fd741dab7b02f4a1ac17f21"
      ],
      "layout": "IPY_MODEL_9c6d5ee3e13d449296a13096ea81f7db"
     }
    },
    "fb70a01286694e1fb40dfd6c940728e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5543bfa403a14af7a572f943d0359336",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e615aa96bb8d461cb6da7384d8180737",
      "value": " 51/51 [00:14&lt;00:00,  3.39it/s]"
     }
    },
    "fea860f9ed9246189b71717b9b13be20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    }
   }
  },
  "interpreter": {
   "hash": "44c3f2fb6a6186031dbdd7e259bb7aac01d91a1126256bc5b90cd38b79422bde"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}